# CrazyFlie-Control-RL

A modular framework for reinforcement learning-based control of CrazyFlie drones in PyBullet environments.

---

## ðŸ“¦ Project Structure

```
gym_pybullet_drones/
â”‚
â”œâ”€â”€ assets/                # Drone models, trajectories, trained agent (to be added)
â”œâ”€â”€ envs/                  # Custom Gymnasium environments
â”œâ”€â”€ control/               # Control algorithms (PID, RL, etc.)
â”œâ”€â”€ utils/                 # Logging, plotting, and helper functions
â”œâ”€â”€ examples/              # Training and evaluation scripts
â””â”€â”€ README.md
```

---

## ðŸš€ Getting Started

### 1. Installation

Clone the repository and install dependencies:

```sh
git clone https://github.com/Pxlsko/CrazyFlie-Control-RL.git
cd CrazyFlie-Control-RL

conda create -n drones python=3.10
conda activate drones

pip install --upgrade pip
pip install -e .
```

### 2. Training an Agent

To train a reinforcement learning agent:

```sh
cd gym_pybullet_drones/examples
python learn.py
```

- Training parameters and environments can be adjusted in `learn.py`.
- The trained agent will be saved in the `results/name` folder.


### 3. Evaluating a Trained Agent

To evaluate a trained policy:

```sh
cd gym_pybullet_drones/examples
python evaluation.py
```

- The script loads the agent from `results/` and runs it in the selected environment.
- Results and plots will show up at the end of the simulation and saved in the path that you have selected in `plot_custom` in `Logger.py` script.

---

## ðŸ§© Environments

- **RandomPointAviary**: Training environment which is used to teach the agent to reach a random target position.
- **TestRandomPointAviary**: Test environment for trajectory following or hover at multiple waypoints. You can select `circle`, `square`, `random`, `helix` or `random_and_hover` inside of the script.
- **ConstantPerturbation**: Test environment to reject a constant external force.
- **ImpactPerturbation**: Test environment to reject an impact perturbation.

Each environment is customizable via parameters in the example scripts.

---

## ðŸ¤– Trained Agent Policy

The trained agent policy will be included in the `results/name` (`name` has to be the same as you type in `learn.py`) folder as:

- `best_model.zip`: Stable Baselines3 PPO agent.
- `final_model.zip`: Final checkpoint after training.

**Some results of the actual trained agent policy:**


---

## ðŸ“Š Results & Visualization

- Training logs and TensorBoard files are saved in `results/name/tb/`. You can see the training progress by typing in terminal:

```sh
tensorboard --logdir = results\Trajectoryresults\tb # or where you have saved your trained model in learn.py
```

- Evaluation metrics (mean reward, precision at waypoints) are printed and plotted.
- Example plots include position tracking, orientation, and velocity comparisons.

---

## ðŸ“š References

- [Gym-pybullet-drones] (https://github.com/utiasDSL/gym-pybullet-drones.git)
- [PyBullet](https://github.com/bulletphysics/bullet3/)
- [Stable Baselines3](https://github.com/hill-a/stable-baselines)
- [Anaconda](https://www.anaconda.com/)
- [Gymnasium](https://arxiv.org/abs/2407.17032)
- [TensorBoard](https://github.com/tensorflow/tensorboard?tab=readme-ov-file)
- [CrazyFlie](10.1109/ICRA.2011.5980409)

---
